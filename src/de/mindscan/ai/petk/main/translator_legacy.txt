    with tab:
        st.write("### English to japanese translator")
        
        english_input = st.text_input("English",disabled=False, key="en2jp_translator.user_input")
        st.write("Input: " + english_input)
        
        input = {
            "user.input":english_input
            }
        
        template_engine = AIPETKTemplateEngine(None)
        
        # 1st step, first shot translation to japanese
        # let's assume we have this phing codelama model
        model = PhindCodeLama34Bv2(None)
        model_template = model.get_unstructured_prompt_template_with_context_and_pretext()
        #st.code(model_template)
        
        
        # ----------------
        
        st.write("#### Task 1 - 1st shot translation")
        
        en2jp_firstshot = EnglishToJapanese_FirstShotTranslation(None)
        model_task, extra_stopwords = buildModelTask(en2jp_firstshot, model_template, input)
        
        st.write("Query")
        st.code(model_task)
        
        # now execute the model task for a given endpoint and retrieve the answer
        invoker = RemoteApiModelInvoker(None)
            
        endpoint = getConnectionEndpoints()['bigserverOobaboogaEndpoint']
        result_1stshot = invoker.invoke_backend(endpoint, model_task, {
                "extra_stopwords":extra_stopwords
            } )
        
        ## update taskRuntimeEnvironment
        input['task1.task'] = model_task
        input['task1.result'] = result_1stshot['llm.response.content']
        
        st.write("Answer")
        st.code(input['task1.result'])
        
        
        # ----------------------------------------
        
        st.write("#### Task 2 - 1st shot refiner")
        
        en2jp_firstshotrefiner = EnglishToJapanese_FirstShotRefiner(None)
        
        system_prompt = en2jp_firstshotrefiner.get_systemm_prompt()
        query = en2jp_firstshotrefiner.get_task_query()
        context_template = en2jp_firstshotrefiner.get_task_context_template()
        pretext_template = en2jp_firstshotrefiner.get_task_answer_pretext_template()
        extra_stopwords = en2jp_firstshotrefiner.get_extra_stopwords()
        
        context = template_engine.evaluateTemplate(context_template, input)
        pretext = template_engine.evaluateTemplate(pretext_template, input)
        
        # now fill the model template
        task_data = {
            'system.prompt':system_prompt,
            'query':query,
            'context':context,
            'pretext':pretext,
            } 

        model_task = template_engine.evaluateTemplate(model_template, task_data)
        st.write("Query")
        st.code(model_task)
        
        result_1stshotrefiner = invoker.invoke_backend(endpoint, model_task, {
                "extra_stopwords":extra_stopwords
            } )
        
        input['task2.task'] = model_task
        input['task2.result'] = result_1stshotrefiner['llm.response.content']
        
        st.write("Answer")
        st.code(input['task2.result'])
        
        # -----------------------------------
        st.write("#### Task 3 - Extract")
        
        en2jp_extractbestanswer = EnglishToJapanese_BestAnswerJsonExtractor(None)
        
        system_prompt = en2jp_extractbestanswer.get_systemm_prompt()
        query = en2jp_extractbestanswer.get_task_query()
        context_template = en2jp_extractbestanswer.get_task_context_template()
        pretext_template = en2jp_extractbestanswer.get_task_answer_pretext_template()
        extra_stopwords = en2jp_extractbestanswer.get_extra_stopwords()
        
        input['expectedResultStructure'] = {
            "english":"The English translation goes here",
            "japanese":"The best Japanese translation goes here",
            }
        
        input['expectedFullResultStructure'] = {
            "english":"The English translation goes here",
            "japanese":"The best Japanese translation goes here",
            "kana":"The Japanese kana (hiragana) reading for the translation goes here",
            "romaji":"The Jpanese reading for the translation goes here"
            }
        
        
        context = template_engine.evaluateTemplate(context_template, input)
        pretext = template_engine.evaluateTemplate(pretext_template, input)

        # now fill the model template
        task_data = {
            'system.prompt':system_prompt,
            'query':query,
            'context':context,
            'pretext':pretext,
            } 

        
        model_task = template_engine.evaluateTemplate(model_template, task_data)
        st.write("Query")
        st.code(model_task)
        
        result_extractor = invoker.invoke_backend(endpoint, model_task, {
                "extra_stopwords":extra_stopwords
            } )
        
        input['task3.task'] = model_task
        input['task3.result'] = result_extractor['llm.response.content']

        st.write("Answer")
        st.code(result_extractor['llm.response.content'], language="json")
        
        # -----------------------------------
        st.write("#### Task 4 - Proofread Answer")
        
        en2jp_proofreader = EnglishToJapanese_ProofreadBestAnswerAndExtract(None)
        
        system_prompt = en2jp_proofreader.get_systemm_prompt()
        query = en2jp_proofreader.get_task_query()
        context_template = en2jp_proofreader.get_task_context_template()
        pretext_template = en2jp_proofreader.get_task_answer_pretext_template()
        extra_stopwords = en2jp_proofreader.get_extra_stopwords()
        
        context = template_engine.evaluateTemplate(context_template, input)
        pretext = template_engine.evaluateTemplate(pretext_template, input)

        # now fill the model template
        task_data = {
            'system.prompt':system_prompt,
            'query':query,
            'context':context,
            'pretext':pretext,
            } 
        
        model_task = template_engine.evaluateTemplate(model_template, task_data)
        st.write("Query")
        st.code(model_task)

        result_proofread = invoker.invoke_backend(endpoint, model_task, {
                "extra_stopwords":extra_stopwords
            } )
        
        input['task4.task'] = model_task
        input['task4.result'] = result_proofread['llm.response.content']
        
        st.write("Answer")
        st.code(result_proofread['llm.response.content'], language="json")

        # -----------------------------------
        st.write("#### Task 5 - Answer rating")
        
        en2jp_rating = EnglishToJapanese_TranslationRating(None)
        
        system_prompt = en2jp_rating.get_systemm_prompt()
        query = en2jp_rating.get_task_query()
        context_template = en2jp_rating.get_task_context_template()
        pretext_template = en2jp_rating.get_task_answer_pretext_template()
        extra_stopwords = en2jp_rating.get_extra_stopwords()
        
        context = template_engine.evaluateTemplate(context_template, input)
        pretext = template_engine.evaluateTemplate(pretext_template, input)

        # now fill the model template
        task_data = {
            'system.prompt':system_prompt,
            'query':query,
            'context':context,
            'pretext':pretext,
            } 
        
        model_task = template_engine.evaluateTemplate(model_template, task_data)
        st.write("Query")
        st.code(model_task)

        result_proofread = invoker.invoke_backend(endpoint, model_task, {
                "extra_stopwords":extra_stopwords
            } )
        
        input['task5.task'] = model_task
        input['task5.result'] = result_proofread['llm.response.content']
        st.write("Answer")
        st.code(result_proofread['llm.response.content'], language="json")
